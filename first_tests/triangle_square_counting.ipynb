{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mniklas-kemper\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "enable_wandb = True\n",
    "if enable_wandb:\n",
    "    import wandb\n",
    "    wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "import graph_tool as gt\n",
    "import graph_tool.generation as gen\n",
    "import graph_tool.topology as top\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "dataset = Planetoid(root='/tmp/Cora', name='Cora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rings(edge_index, max_k, min_k= 3):\n",
    "    if isinstance(edge_index, torch.Tensor):\n",
    "        edge_index = edge_index.numpy()\n",
    "\n",
    "    edge_list = edge_index.T\n",
    "    graph_gt = gt.Graph(directed=False)\n",
    "    graph_gt.add_edge_list(edge_list)\n",
    "    gen.remove_self_loops(graph_gt)\n",
    "    gen.remove_parallel_edges(graph_gt)\n",
    "    rings = set()\n",
    "    sorted_rings = set()\n",
    "    for k in range(min_k, max_k+1):\n",
    "        pattern = nx.cycle_graph(k)\n",
    "        pattern_edge_list = list(pattern.edges)\n",
    "        pattern_gt = gt.Graph(directed=False)\n",
    "        pattern_gt.add_edge_list(pattern_edge_list)\n",
    "        sub_isos = top.subgraph_isomorphism(pattern_gt, graph_gt, induced=True, subgraph=True,\n",
    "                                           generator=True)\n",
    "        sub_iso_sets = map(lambda isomorphism: tuple(isomorphism.a), sub_isos)\n",
    "        for iso in sub_iso_sets:\n",
    "            if tuple(sorted(iso)) not in sorted_rings:\n",
    "                rings.add(iso)\n",
    "                sorted_rings.add(tuple(sorted(iso)))\n",
    "    rings = list(rings)\n",
    "    return rings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "triangles = get_rings(dataset[0].edge_index, max_k = 3)\n",
    "squares = get_rings(dataset[0].edge_index, max_k = 4, min_k = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "def get_node_counts(substructure):\n",
    "    return torch.tensor([list(chain(*substructure)).count(i) for i in range(dataset[0].num_nodes)], dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "triangle_count = get_node_counts(triangles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "square_count = get_node_counts(squares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_substructure_edge_index(substructure):\n",
    "    return torch.tensor([[node_id, sub_id]  for sub_id, sub in enumerate(substructure) for node_id in sub], dtype = torch.long).t().contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "substructures_edge_index = [get_substructure_edge_index(substructure) for substructure in [triangles, squares]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset[0].update({\"substructures_edge_index\": substructures_edge_index, \"y\": square_count})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/niklas/Documents/Studium/Informatik/Masterarbeit/substructureML/first_tests/wandb/run-20230713_145232-mkyd2z7x</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/niklas-kemper/triangle-square-counting/runs/mkyd2z7x' target=\"_blank\">exalted-aardvark-3</a></strong> to <a href='https://wandb.ai/niklas-kemper/triangle-square-counting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/niklas-kemper/triangle-square-counting' target=\"_blank\">https://wandb.ai/niklas-kemper/triangle-square-counting</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/niklas-kemper/triangle-square-counting/runs/mkyd2z7x' target=\"_blank\">https://wandb.ai/niklas-kemper/triangle-square-counting/runs/mkyd2z7x</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if enable_wandb:\n",
    "    wandb.init(project='triangle-square-counting')\n",
    "    summary = dict()\n",
    "    summary[\"data\"] = dict()\n",
    "    summary[\"data\"][\"num_features\"] = data.num_features\n",
    "    summary[\"data\"][\"num_nodes\"] = data.num_nodes\n",
    "    summary[\"data\"][\"num_edges\"] = data.num_edges \n",
    "    summary[\"data\"][\"has_isolated_nodes\"] = data.has_isolated_nodes()\n",
    "    summary[\"data\"][\"has_self_nodes\"] = data.has_self_loops()\n",
    "    summary[\"data\"][\"is_undirected\"] = data.is_undirected()\n",
    "    summary[\"data\"][\"num_training_nodes\"] = data.train_mask.sum()\n",
    "    wandb.summary = summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simple_model_triangles import SimpleSubstructureNeuralNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleSubstructureNeuralNet(\n",
      "  (layers): ModuleList(\n",
      "    (0): SubstructureLayer(\n",
      "      (message_neighbor): GINConv(nn=Sequential(\n",
      "        (0): Linear(in_features=1433, out_features=32, bias=True)\n",
      "        (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): Linear(in_features=32, out_features=16, bias=True)\n",
      "      ))\n",
      "      (node2substructures): ModuleList(\n",
      "        (0-1): 2 x Sequential(\n",
      "          (0): Linear(in_features=16, out_features=32, bias=True)\n",
      "          (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "          (3): Linear(in_features=32, out_features=16, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (substructures2node): ModuleList(\n",
      "        (0-1): 2 x Sequential(\n",
      "          (0): Linear(in_features=16, out_features=32, bias=True)\n",
      "          (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "          (3): Linear(in_features=32, out_features=16, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1-2): 2 x SubstructureLayer(\n",
      "      (message_neighbor): GINConv(nn=Sequential(\n",
      "        (0): Linear(in_features=16, out_features=32, bias=True)\n",
      "        (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): Linear(in_features=32, out_features=16, bias=True)\n",
      "      ))\n",
      "      (node2substructures): ModuleList(\n",
      "        (0-1): 2 x Sequential(\n",
      "          (0): Linear(in_features=16, out_features=32, bias=True)\n",
      "          (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "          (3): Linear(in_features=32, out_features=16, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (substructures2node): ModuleList(\n",
      "        (0-1): 2 x Sequential(\n",
      "          (0): Linear(in_features=16, out_features=32, bias=True)\n",
      "          (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "          (3): Linear(in_features=32, out_features=16, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lin): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = SimpleSubstructureNeuralNet(hidden_channels = 16, out_channels = 1,in_channels = dataset.num_node_features, num_layers = 3, num_substructures=2)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Embedding, ModuleList\n",
    "from torch.nn import Sequential, Linear, BatchNorm1d, ReLU\n",
    "#from torch_scatter import scatter\n",
    "from torch_geometric.nn import GINConv, GINEConv\n",
    "\n",
    "class SimpleGraphNeuralNet(torch.nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels, out_channels, num_layers):\n",
    "        \n",
    "        super(SimpleGraphNeuralNet, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.node_convs = ModuleList()\n",
    "\n",
    "        for layer in range(num_layers):\n",
    "            if layer == 0:\n",
    "                nn = Sequential(\n",
    "                Linear(input_channels, 2 * hidden_channels),\n",
    "                BatchNorm1d(2 * hidden_channels),\n",
    "                ReLU(),\n",
    "                Linear(2 * hidden_channels, hidden_channels),\n",
    "            )\n",
    "            else:\n",
    "                nn = Sequential(\n",
    "                    Linear(hidden_channels, 2 * hidden_channels),\n",
    "                    BatchNorm1d(2 * hidden_channels),\n",
    "                    ReLU(),\n",
    "                    Linear(2 * hidden_channels, hidden_channels),\n",
    "                )\n",
    "            self.node_convs.append(GINConv(nn, train_eps=True))\n",
    "\n",
    "        self.lin = Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "\n",
    "        for conv in self.node_convs:\n",
    "            conv.reset_parameters()\n",
    "\n",
    "        self.lin.reset_parameters()\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "\n",
    "            x = self.node_convs[i](x=x, edge_index=data.edge_index)\n",
    "\n",
    "\n",
    "        #x = scatter(x, data.batch, dim=0, reduce='mean')\n",
    "        #x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.lin(x)\n",
    "\n",
    "\n",
    "        #x = F.relu(x)\n",
    "        #x = F.dropout(x, self.dropout, training=self.training)\n",
    "        #x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "model2 = SimpleGraphNeuralNet(dataset.num_node_features, 16, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeefffced8e140858175fb755e53af7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016683225599998273, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20230717_115007-x4tfc9zg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/niklas-kemper/triangle-square-counting/runs/x4tfc9zg' target=\"_blank\">square counting</a></strong> to <a href='https://wandb.ai/niklas-kemper/triangle-square-counting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/niklas-kemper/triangle-square-counting' target=\"_blank\">https://wandb.ai/niklas-kemper/triangle-square-counting</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/niklas-kemper/triangle-square-counting/runs/x4tfc9zg' target=\"_blank\">https://wandb.ai/niklas-kemper/triangle-square-counting/runs/x4tfc9zg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type                        | Params\n",
      "------------------------------------------------------\n",
      "0 | model | SimpleSubstructureNeuralNet | 62.4 K\n",
      "------------------------------------------------------\n",
      "62.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "62.4 K    Total params\n",
      "0.250     Total estimated model params size (MB)\n",
      "/Users/niklas/anaconda3/envs/substructureML/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/niklas/anaconda3/envs/substructureML/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f1db46dab324652b870dee5a31e5a8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>training_loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▃▁▁▁▆▆▆▇▇▇▇▇▇▇▇█████████████████████████</td></tr><tr><td>val_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>trainer/global_step</td><td>199</td></tr><tr><td>training_loss</td><td>0.00179</td></tr><tr><td>val_acc</td><td>0.95</td></tr><tr><td>val_loss</td><td>0.16753</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">square counting</strong> at: <a href='https://wandb.ai/niklas-kemper/triangle-square-counting/runs/x4tfc9zg' target=\"_blank\">https://wandb.ai/niklas-kemper/triangle-square-counting/runs/x4tfc9zg</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230717_115007-x4tfc9zg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trainer import GraphNet\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning import Trainer\n",
    "from torch_geometric.loader import DataLoader\n",
    "wandb_logger = WandbLogger(project='triangle-square-counting', name= \"square counting\")\n",
    "\n",
    "train_loader = DataLoader([data])\n",
    "val_loader = DataLoader([data])\n",
    "test_loader = DataLoader([data])\n",
    "\n",
    "trainer = Trainer(max_epochs=200, logger= wandb_logger, enable_progress_bar= False, log_every_n_steps=1)\n",
    "model_lightning = GraphNet(model)\n",
    "\n",
    "trainer.fit(model_lightning, train_dataloaders=train_loader, val_dataloaders= val_loader)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 22.6870, Test Loss: 1377639.5000, Test Acc: 0.3380\n",
      "Epoch: 002, Loss: 18.0280, Test Loss: 2134068.5000, Test Acc: 0.3210\n",
      "Epoch: 003, Loss: 13.9008, Test Loss: 2130215.7500, Test Acc: 0.3230\n",
      "Epoch: 004, Loss: 9.1008, Test Loss: 7259279.5000, Test Acc: 0.1220\n",
      "Epoch: 005, Loss: 4.8206, Test Loss: 4833816.5000, Test Acc: 0.1040\n",
      "Epoch: 006, Loss: 3.1381, Test Loss: 4838285.0000, Test Acc: 0.1160\n",
      "Epoch: 007, Loss: 2.8995, Test Loss: 3538408.5000, Test Acc: 0.1120\n",
      "Epoch: 008, Loss: 2.6522, Test Loss: 1471323.1250, Test Acc: 0.0930\n",
      "Epoch: 009, Loss: 2.1090, Test Loss: 564561.2500, Test Acc: 0.1040\n",
      "Epoch: 010, Loss: 1.8465, Test Loss: 214053.5938, Test Acc: 0.1160\n",
      "Epoch: 011, Loss: 1.5182, Test Loss: 85462.3906, Test Acc: 0.1440\n",
      "Epoch: 012, Loss: 1.1052, Test Loss: 34237.3672, Test Acc: 0.1580\n",
      "Epoch: 013, Loss: 0.9520, Test Loss: 13829.2236, Test Acc: 0.2050\n",
      "Epoch: 014, Loss: 0.7205, Test Loss: 6009.2241, Test Acc: 0.2380\n",
      "Epoch: 015, Loss: 0.6002, Test Loss: 2858.5491, Test Acc: 0.2640\n",
      "Epoch: 016, Loss: 0.4442, Test Loss: 1441.0563, Test Acc: 0.2770\n",
      "Epoch: 017, Loss: 0.3440, Test Loss: 791.1212, Test Acc: 0.4390\n",
      "Epoch: 018, Loss: 0.2960, Test Loss: 463.4486, Test Acc: 0.4970\n",
      "Epoch: 019, Loss: 0.2612, Test Loss: 278.4275, Test Acc: 0.5710\n",
      "Epoch: 020, Loss: 0.2267, Test Loss: 168.8629, Test Acc: 0.5850\n",
      "Epoch: 021, Loss: 0.2336, Test Loss: 108.0270, Test Acc: 0.6310\n",
      "Epoch: 022, Loss: 0.2144, Test Loss: 73.6836, Test Acc: 0.6380\n",
      "Epoch: 023, Loss: 0.2207, Test Loss: 53.0676, Test Acc: 0.6430\n",
      "Epoch: 024, Loss: 0.2321, Test Loss: 39.6422, Test Acc: 0.6550\n",
      "Epoch: 025, Loss: 0.2282, Test Loss: 30.1654, Test Acc: 0.6630\n",
      "Epoch: 026, Loss: 0.2294, Test Loss: 23.5015, Test Acc: 0.6660\n",
      "Epoch: 027, Loss: 0.2256, Test Loss: 18.1740, Test Acc: 0.6730\n",
      "Epoch: 028, Loss: 0.2172, Test Loss: 13.8655, Test Acc: 0.6770\n",
      "Epoch: 029, Loss: 0.2050, Test Loss: 10.6149, Test Acc: 0.6760\n",
      "Epoch: 030, Loss: 0.1931, Test Loss: 8.4459, Test Acc: 0.6830\n",
      "Epoch: 031, Loss: 0.1774, Test Loss: 7.3627, Test Acc: 0.6950\n",
      "Epoch: 032, Loss: 0.1623, Test Loss: 6.7880, Test Acc: 0.7060\n",
      "Epoch: 033, Loss: 0.1503, Test Loss: 6.0583, Test Acc: 0.7100\n",
      "Epoch: 034, Loss: 0.1382, Test Loss: 4.7906, Test Acc: 0.7180\n",
      "Epoch: 035, Loss: 0.1281, Test Loss: 3.6482, Test Acc: 0.7260\n",
      "Epoch: 036, Loss: 0.1241, Test Loss: 3.8517, Test Acc: 0.7390\n",
      "Epoch: 037, Loss: 0.1584, Test Loss: 1.4865, Test Acc: 0.7470\n",
      "Epoch: 038, Loss: 0.1979, Test Loss: 1.2700, Test Acc: 0.7530\n",
      "Epoch: 039, Loss: 0.1760, Test Loss: 1.3009, Test Acc: 0.7720\n",
      "Epoch: 040, Loss: 0.1143, Test Loss: 1.3677, Test Acc: 0.7880\n",
      "Epoch: 041, Loss: 0.1507, Test Loss: 1.2912, Test Acc: 0.7970\n",
      "Epoch: 042, Loss: 0.1555, Test Loss: 1.0479, Test Acc: 0.8060\n",
      "Epoch: 043, Loss: 0.0925, Test Loss: 0.8292, Test Acc: 0.8060\n",
      "Epoch: 044, Loss: 0.0893, Test Loss: 0.7172, Test Acc: 0.8140\n",
      "Epoch: 045, Loss: 0.1223, Test Loss: 0.6757, Test Acc: 0.8230\n",
      "Epoch: 046, Loss: 0.0866, Test Loss: 0.6636, Test Acc: 0.8370\n",
      "Epoch: 047, Loss: 0.0678, Test Loss: 0.6309, Test Acc: 0.8460\n",
      "Epoch: 048, Loss: 0.0908, Test Loss: 0.5573, Test Acc: 0.8510\n",
      "Epoch: 049, Loss: 0.0842, Test Loss: 0.4247, Test Acc: 0.8590\n",
      "Epoch: 050, Loss: 0.0573, Test Loss: 0.3334, Test Acc: 0.8620\n",
      "Epoch: 051, Loss: 0.0622, Test Loss: 0.2734, Test Acc: 0.8680\n",
      "Epoch: 052, Loss: 0.0705, Test Loss: 0.2349, Test Acc: 0.8800\n",
      "Epoch: 053, Loss: 0.0530, Test Loss: 0.2101, Test Acc: 0.8940\n",
      "Epoch: 054, Loss: 0.0429, Test Loss: 0.1910, Test Acc: 0.9010\n",
      "Epoch: 055, Loss: 0.0520, Test Loss: 0.1748, Test Acc: 0.9100\n",
      "Epoch: 056, Loss: 0.0504, Test Loss: 0.1592, Test Acc: 0.9200\n",
      "Epoch: 057, Loss: 0.0365, Test Loss: 0.1520, Test Acc: 0.9210\n",
      "Epoch: 058, Loss: 0.0366, Test Loss: 0.1534, Test Acc: 0.9250\n",
      "Epoch: 059, Loss: 0.0427, Test Loss: 0.1604, Test Acc: 0.9230\n",
      "Epoch: 060, Loss: 0.0347, Test Loss: 0.1680, Test Acc: 0.9240\n",
      "Epoch: 061, Loss: 0.0299, Test Loss: 0.1639, Test Acc: 0.9230\n",
      "Epoch: 062, Loss: 0.0339, Test Loss: 0.1415, Test Acc: 0.9320\n",
      "Epoch: 063, Loss: 0.0302, Test Loss: 0.1139, Test Acc: 0.9440\n",
      "Epoch: 064, Loss: 0.0234, Test Loss: 0.0924, Test Acc: 0.9460\n",
      "Epoch: 065, Loss: 0.0246, Test Loss: 0.0819, Test Acc: 0.9490\n",
      "Epoch: 066, Loss: 0.0259, Test Loss: 0.0776, Test Acc: 0.9550\n",
      "Epoch: 067, Loss: 0.0222, Test Loss: 0.0750, Test Acc: 0.9570\n",
      "Epoch: 068, Loss: 0.0208, Test Loss: 0.0776, Test Acc: 0.9570\n",
      "Epoch: 069, Loss: 0.0206, Test Loss: 0.0846, Test Acc: 0.9560\n",
      "Epoch: 070, Loss: 0.0181, Test Loss: 0.0924, Test Acc: 0.9550\n",
      "Epoch: 071, Loss: 0.0164, Test Loss: 0.0970, Test Acc: 0.9550\n",
      "Epoch: 072, Loss: 0.0178, Test Loss: 0.0966, Test Acc: 0.9570\n",
      "Epoch: 073, Loss: 0.0176, Test Loss: 0.0909, Test Acc: 0.9590\n",
      "Epoch: 074, Loss: 0.0158, Test Loss: 0.0813, Test Acc: 0.9620\n",
      "Epoch: 075, Loss: 0.0157, Test Loss: 0.0705, Test Acc: 0.9640\n",
      "Epoch: 076, Loss: 0.0151, Test Loss: 0.0624, Test Acc: 0.9640\n",
      "Epoch: 077, Loss: 0.0134, Test Loss: 0.0577, Test Acc: 0.9680\n",
      "Epoch: 078, Loss: 0.0132, Test Loss: 0.0547, Test Acc: 0.9690\n",
      "Epoch: 079, Loss: 0.0135, Test Loss: 0.0522, Test Acc: 0.9680\n",
      "Epoch: 080, Loss: 0.0126, Test Loss: 0.0504, Test Acc: 0.9700\n",
      "Epoch: 081, Loss: 0.0123, Test Loss: 0.0500, Test Acc: 0.9710\n",
      "Epoch: 082, Loss: 0.0122, Test Loss: 0.0513, Test Acc: 0.9720\n",
      "Epoch: 083, Loss: 0.0112, Test Loss: 0.0530, Test Acc: 0.9710\n",
      "Epoch: 084, Loss: 0.0109, Test Loss: 0.0535, Test Acc: 0.9680\n",
      "Epoch: 085, Loss: 0.0110, Test Loss: 0.0524, Test Acc: 0.9680\n",
      "Epoch: 086, Loss: 0.0105, Test Loss: 0.0501, Test Acc: 0.9710\n",
      "Epoch: 087, Loss: 0.0102, Test Loss: 0.0477, Test Acc: 0.9740\n",
      "Epoch: 088, Loss: 0.0101, Test Loss: 0.0457, Test Acc: 0.9750\n",
      "Epoch: 089, Loss: 0.0097, Test Loss: 0.0442, Test Acc: 0.9750\n",
      "Epoch: 090, Loss: 0.0092, Test Loss: 0.0431, Test Acc: 0.9750\n",
      "Epoch: 091, Loss: 0.0092, Test Loss: 0.0422, Test Acc: 0.9770\n",
      "Epoch: 092, Loss: 0.0089, Test Loss: 0.0416, Test Acc: 0.9780\n",
      "Epoch: 093, Loss: 0.0086, Test Loss: 0.0416, Test Acc: 0.9770\n",
      "Epoch: 094, Loss: 0.0084, Test Loss: 0.0422, Test Acc: 0.9770\n",
      "Epoch: 095, Loss: 0.0082, Test Loss: 0.0432, Test Acc: 0.9760\n",
      "Epoch: 096, Loss: 0.0080, Test Loss: 0.0437, Test Acc: 0.9770\n",
      "Epoch: 097, Loss: 0.0079, Test Loss: 0.0430, Test Acc: 0.9780\n",
      "Epoch: 098, Loss: 0.0077, Test Loss: 0.0414, Test Acc: 0.9800\n",
      "Epoch: 099, Loss: 0.0074, Test Loss: 0.0396, Test Acc: 0.9800\n",
      "Epoch: 100, Loss: 0.0072, Test Loss: 0.0386, Test Acc: 0.9810\n",
      "Epoch: 101, Loss: 0.0071, Test Loss: 0.0386, Test Acc: 0.9810\n",
      "Epoch: 102, Loss: 0.0069, Test Loss: 0.0390, Test Acc: 0.9810\n",
      "Epoch: 103, Loss: 0.0068, Test Loss: 0.0391, Test Acc: 0.9800\n",
      "Epoch: 104, Loss: 0.0066, Test Loss: 0.0388, Test Acc: 0.9790\n",
      "Epoch: 105, Loss: 0.0064, Test Loss: 0.0384, Test Acc: 0.9800\n",
      "Epoch: 106, Loss: 0.0064, Test Loss: 0.0381, Test Acc: 0.9810\n",
      "Epoch: 107, Loss: 0.0062, Test Loss: 0.0378, Test Acc: 0.9810\n",
      "Epoch: 108, Loss: 0.0060, Test Loss: 0.0371, Test Acc: 0.9800\n",
      "Epoch: 109, Loss: 0.0059, Test Loss: 0.0362, Test Acc: 0.9820\n",
      "Epoch: 110, Loss: 0.0058, Test Loss: 0.0357, Test Acc: 0.9840\n",
      "Epoch: 111, Loss: 0.0057, Test Loss: 0.0359, Test Acc: 0.9840\n",
      "Epoch: 112, Loss: 0.0055, Test Loss: 0.0363, Test Acc: 0.9830\n",
      "Epoch: 113, Loss: 0.0054, Test Loss: 0.0364, Test Acc: 0.9830\n",
      "Epoch: 114, Loss: 0.0053, Test Loss: 0.0357, Test Acc: 0.9830\n",
      "Epoch: 115, Loss: 0.0052, Test Loss: 0.0346, Test Acc: 0.9830\n",
      "Epoch: 116, Loss: 0.0050, Test Loss: 0.0337, Test Acc: 0.9850\n",
      "Epoch: 117, Loss: 0.0050, Test Loss: 0.0333, Test Acc: 0.9850\n",
      "Epoch: 118, Loss: 0.0048, Test Loss: 0.0331, Test Acc: 0.9840\n",
      "Epoch: 119, Loss: 0.0047, Test Loss: 0.0329, Test Acc: 0.9840\n",
      "Epoch: 120, Loss: 0.0046, Test Loss: 0.0327, Test Acc: 0.9840\n",
      "Epoch: 121, Loss: 0.0045, Test Loss: 0.0323, Test Acc: 0.9840\n",
      "Epoch: 122, Loss: 0.0044, Test Loss: 0.0318, Test Acc: 0.9840\n",
      "Epoch: 123, Loss: 0.0044, Test Loss: 0.0313, Test Acc: 0.9840\n",
      "Epoch: 124, Loss: 0.0043, Test Loss: 0.0308, Test Acc: 0.9850\n",
      "Epoch: 125, Loss: 0.0042, Test Loss: 0.0303, Test Acc: 0.9850\n",
      "Epoch: 126, Loss: 0.0041, Test Loss: 0.0297, Test Acc: 0.9850\n",
      "Epoch: 127, Loss: 0.0040, Test Loss: 0.0292, Test Acc: 0.9850\n",
      "Epoch: 128, Loss: 0.0039, Test Loss: 0.0290, Test Acc: 0.9850\n",
      "Epoch: 129, Loss: 0.0038, Test Loss: 0.0288, Test Acc: 0.9850\n",
      "Epoch: 130, Loss: 0.0038, Test Loss: 0.0286, Test Acc: 0.9850\n",
      "Epoch: 131, Loss: 0.0037, Test Loss: 0.0285, Test Acc: 0.9850\n",
      "Epoch: 132, Loss: 0.0036, Test Loss: 0.0284, Test Acc: 0.9850\n",
      "Epoch: 133, Loss: 0.0035, Test Loss: 0.0284, Test Acc: 0.9850\n",
      "Epoch: 134, Loss: 0.0035, Test Loss: 0.0282, Test Acc: 0.9850\n",
      "Epoch: 135, Loss: 0.0034, Test Loss: 0.0280, Test Acc: 0.9850\n",
      "Epoch: 136, Loss: 0.0034, Test Loss: 0.0278, Test Acc: 0.9850\n",
      "Epoch: 137, Loss: 0.0033, Test Loss: 0.0275, Test Acc: 0.9850\n",
      "Epoch: 138, Loss: 0.0032, Test Loss: 0.0272, Test Acc: 0.9850\n",
      "Epoch: 139, Loss: 0.0032, Test Loss: 0.0269, Test Acc: 0.9850\n",
      "Epoch: 140, Loss: 0.0031, Test Loss: 0.0268, Test Acc: 0.9850\n",
      "Epoch: 141, Loss: 0.0031, Test Loss: 0.0267, Test Acc: 0.9850\n",
      "Epoch: 142, Loss: 0.0030, Test Loss: 0.0265, Test Acc: 0.9850\n",
      "Epoch: 143, Loss: 0.0030, Test Loss: 0.0265, Test Acc: 0.9850\n",
      "Epoch: 144, Loss: 0.0029, Test Loss: 0.0264, Test Acc: 0.9850\n",
      "Epoch: 145, Loss: 0.0029, Test Loss: 0.0264, Test Acc: 0.9850\n",
      "Epoch: 146, Loss: 0.0028, Test Loss: 0.0263, Test Acc: 0.9850\n",
      "Epoch: 147, Loss: 0.0028, Test Loss: 0.0261, Test Acc: 0.9850\n",
      "Epoch: 148, Loss: 0.0027, Test Loss: 0.0259, Test Acc: 0.9850\n",
      "Epoch: 149, Loss: 0.0027, Test Loss: 0.0257, Test Acc: 0.9850\n",
      "Epoch: 150, Loss: 0.0026, Test Loss: 0.0255, Test Acc: 0.9850\n",
      "Epoch: 151, Loss: 0.0026, Test Loss: 0.0253, Test Acc: 0.9850\n",
      "Epoch: 152, Loss: 0.0025, Test Loss: 0.0252, Test Acc: 0.9850\n",
      "Epoch: 153, Loss: 0.0025, Test Loss: 0.0251, Test Acc: 0.9850\n",
      "Epoch: 154, Loss: 0.0025, Test Loss: 0.0250, Test Acc: 0.9850\n",
      "Epoch: 155, Loss: 0.0024, Test Loss: 0.0249, Test Acc: 0.9850\n",
      "Epoch: 156, Loss: 0.0024, Test Loss: 0.0248, Test Acc: 0.9850\n",
      "Epoch: 157, Loss: 0.0024, Test Loss: 0.0247, Test Acc: 0.9850\n",
      "Epoch: 158, Loss: 0.0023, Test Loss: 0.0245, Test Acc: 0.9850\n",
      "Epoch: 159, Loss: 0.0023, Test Loss: 0.0244, Test Acc: 0.9850\n",
      "Epoch: 160, Loss: 0.0023, Test Loss: 0.0243, Test Acc: 0.9850\n",
      "Epoch: 161, Loss: 0.0022, Test Loss: 0.0242, Test Acc: 0.9850\n",
      "Epoch: 162, Loss: 0.0022, Test Loss: 0.0241, Test Acc: 0.9850\n",
      "Epoch: 163, Loss: 0.0022, Test Loss: 0.0240, Test Acc: 0.9850\n",
      "Epoch: 164, Loss: 0.0021, Test Loss: 0.0239, Test Acc: 0.9850\n",
      "Epoch: 165, Loss: 0.0021, Test Loss: 0.0237, Test Acc: 0.9850\n",
      "Epoch: 166, Loss: 0.0021, Test Loss: 0.0236, Test Acc: 0.9850\n",
      "Epoch: 167, Loss: 0.0021, Test Loss: 0.0235, Test Acc: 0.9850\n",
      "Epoch: 168, Loss: 0.0020, Test Loss: 0.0234, Test Acc: 0.9850\n",
      "Epoch: 169, Loss: 0.0020, Test Loss: 0.0232, Test Acc: 0.9850\n",
      "Epoch: 170, Loss: 0.0020, Test Loss: 0.0231, Test Acc: 0.9850\n",
      "Epoch: 171, Loss: 0.0019, Test Loss: 0.0230, Test Acc: 0.9850\n",
      "Epoch: 172, Loss: 0.0019, Test Loss: 0.0229, Test Acc: 0.9850\n",
      "Epoch: 173, Loss: 0.0019, Test Loss: 0.0229, Test Acc: 0.9850\n",
      "Epoch: 174, Loss: 0.0019, Test Loss: 0.0228, Test Acc: 0.9850\n",
      "Epoch: 175, Loss: 0.0018, Test Loss: 0.0227, Test Acc: 0.9850\n",
      "Epoch: 176, Loss: 0.0018, Test Loss: 0.0226, Test Acc: 0.9850\n",
      "Epoch: 177, Loss: 0.0018, Test Loss: 0.0225, Test Acc: 0.9850\n",
      "Epoch: 178, Loss: 0.0018, Test Loss: 0.0224, Test Acc: 0.9850\n",
      "Epoch: 179, Loss: 0.0017, Test Loss: 0.0222, Test Acc: 0.9850\n",
      "Epoch: 180, Loss: 0.0017, Test Loss: 0.0221, Test Acc: 0.9850\n",
      "Epoch: 181, Loss: 0.0017, Test Loss: 0.0220, Test Acc: 0.9850\n",
      "Epoch: 182, Loss: 0.0017, Test Loss: 0.0220, Test Acc: 0.9850\n",
      "Epoch: 183, Loss: 0.0016, Test Loss: 0.0219, Test Acc: 0.9850\n",
      "Epoch: 184, Loss: 0.0016, Test Loss: 0.0218, Test Acc: 0.9850\n",
      "Epoch: 185, Loss: 0.0016, Test Loss: 0.0217, Test Acc: 0.9850\n",
      "Epoch: 186, Loss: 0.0016, Test Loss: 0.0216, Test Acc: 0.9850\n",
      "Epoch: 187, Loss: 0.0016, Test Loss: 0.0215, Test Acc: 0.9860\n",
      "Epoch: 188, Loss: 0.0015, Test Loss: 0.0214, Test Acc: 0.9860\n",
      "Epoch: 189, Loss: 0.0015, Test Loss: 0.0213, Test Acc: 0.9860\n",
      "Epoch: 190, Loss: 0.0015, Test Loss: 0.0212, Test Acc: 0.9860\n",
      "Epoch: 191, Loss: 0.0015, Test Loss: 0.0211, Test Acc: 0.9860\n",
      "Epoch: 192, Loss: 0.0015, Test Loss: 0.0210, Test Acc: 0.9860\n",
      "Epoch: 193, Loss: 0.0014, Test Loss: 0.0208, Test Acc: 0.9860\n",
      "Epoch: 194, Loss: 0.0014, Test Loss: 0.0208, Test Acc: 0.9860\n",
      "Epoch: 195, Loss: 0.0014, Test Loss: 0.0207, Test Acc: 0.9860\n",
      "Epoch: 196, Loss: 0.0014, Test Loss: 0.0206, Test Acc: 0.9860\n",
      "Epoch: 197, Loss: 0.0014, Test Loss: 0.0205, Test Acc: 0.9860\n",
      "Epoch: 198, Loss: 0.0013, Test Loss: 0.0204, Test Acc: 0.9860\n",
      "Epoch: 199, Loss: 0.0013, Test Loss: 0.0203, Test Acc: 0.9860\n",
      "Epoch: 200, Loss: 0.0013, Test Loss: 0.0202, Test Acc: 0.9860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ConnectionError), entering retry loop.\n"
     ]
    }
   ],
   "source": [
    "criterion = F.mse_loss  # Define loss criterion.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)  # Define optimizer.\n",
    "\n",
    "def train():\n",
    "      model.train()\n",
    "      optimizer.zero_grad()  # Clear gradients.\n",
    "      out = torch.squeeze(model(data))  # Perform a single forward pass.\n",
    "      loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss solely based on the training nodes.\n",
    "      loss.backward()  # Derive gradients.\n",
    "      optimizer.step()  # Update parameters based on gradients.\n",
    "      return loss\n",
    "\n",
    "def test():\n",
    "      model.eval()\n",
    "      out = torch.squeeze(model(data))\n",
    "      pred = torch.round(out)  # Round to integer\n",
    "      test_loss = criterion(out[data.test_mask], data.y[data.test_mask])\n",
    "      test_correct = pred[data.test_mask] == data.y[data.test_mask]  # Check against ground-truth labels.\n",
    "      test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  # Derive ratio of correct predictions.\n",
    "      return test_acc, test_loss\n",
    "\n",
    "#model.reset_parameters()\n",
    "for epoch in range(1, 201):\n",
    "    loss = train()\n",
    "    test_acc, test_loss = test()\n",
    "    if enable_wandb:\n",
    "        wandb.log({\"substructureGNN/loss\": loss, \"substructureGNN/test_loss\": test_loss, \"substructureGNN/test_acc\": test_acc})\n",
    "    \n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Loss: 24.1846\n",
      "Epoch: 001, Loss: 21.5803\n",
      "Epoch: 002, Loss: 18.1493\n",
      "Epoch: 003, Loss: 13.4987\n",
      "Epoch: 004, Loss: 10.6048\n",
      "Epoch: 005, Loss: 7.6783\n",
      "Epoch: 006, Loss: 4.2272\n",
      "Epoch: 007, Loss: 1.4557\n",
      "Epoch: 008, Loss: 1.3588\n",
      "Epoch: 009, Loss: 1.6830\n",
      "Epoch: 010, Loss: 1.8849\n",
      "Epoch: 011, Loss: 1.7098\n",
      "Epoch: 012, Loss: 1.4532\n",
      "Epoch: 013, Loss: 1.2828\n",
      "Epoch: 014, Loss: 1.0259\n",
      "Epoch: 015, Loss: 0.8474\n",
      "Epoch: 016, Loss: 0.6811\n",
      "Epoch: 017, Loss: 0.6298\n",
      "Epoch: 018, Loss: 0.5018\n",
      "Epoch: 019, Loss: 0.4021\n",
      "Epoch: 020, Loss: 0.3520\n",
      "Epoch: 021, Loss: 0.3064\n",
      "Epoch: 022, Loss: 0.2621\n",
      "Epoch: 023, Loss: 0.2384\n",
      "Epoch: 024, Loss: 0.2200\n",
      "Epoch: 025, Loss: 0.2020\n",
      "Epoch: 026, Loss: 0.1927\n",
      "Epoch: 027, Loss: 0.1865\n",
      "Epoch: 028, Loss: 0.1797\n",
      "Epoch: 029, Loss: 0.1806\n",
      "Epoch: 030, Loss: 0.1825\n",
      "Epoch: 031, Loss: 0.1733\n",
      "Epoch: 032, Loss: 0.1699\n",
      "Epoch: 033, Loss: 0.1586\n",
      "Epoch: 034, Loss: 0.1498\n",
      "Epoch: 035, Loss: 0.1391\n",
      "Epoch: 036, Loss: 0.1285\n",
      "Epoch: 037, Loss: 0.1212\n",
      "Epoch: 038, Loss: 0.1097\n",
      "Epoch: 039, Loss: 0.1026\n",
      "Epoch: 040, Loss: 0.0934\n",
      "Epoch: 041, Loss: 0.0893\n",
      "Epoch: 042, Loss: 0.0820\n",
      "Epoch: 043, Loss: 0.0783\n",
      "Epoch: 044, Loss: 0.0729\n",
      "Epoch: 045, Loss: 0.0710\n",
      "Epoch: 046, Loss: 0.0650\n",
      "Epoch: 047, Loss: 0.0595\n",
      "Epoch: 048, Loss: 0.0578\n",
      "Epoch: 049, Loss: 0.0538\n",
      "Epoch: 050, Loss: 0.0517\n",
      "Epoch: 051, Loss: 0.0483\n",
      "Epoch: 052, Loss: 0.0486\n",
      "Epoch: 053, Loss: 0.0443\n",
      "Epoch: 054, Loss: 0.0391\n",
      "Epoch: 055, Loss: 0.0391\n",
      "Epoch: 056, Loss: 0.0357\n",
      "Epoch: 057, Loss: 0.0347\n",
      "Epoch: 058, Loss: 0.0326\n",
      "Epoch: 059, Loss: 0.0305\n",
      "Epoch: 060, Loss: 0.0291\n",
      "Epoch: 061, Loss: 0.0269\n",
      "Epoch: 062, Loss: 0.0265\n",
      "Epoch: 063, Loss: 0.0244\n",
      "Epoch: 064, Loss: 0.0241\n",
      "Epoch: 065, Loss: 0.0221\n",
      "Epoch: 066, Loss: 0.0214\n",
      "Epoch: 067, Loss: 0.0203\n",
      "Epoch: 068, Loss: 0.0193\n",
      "Epoch: 069, Loss: 0.0189\n",
      "Epoch: 070, Loss: 0.0175\n",
      "Epoch: 071, Loss: 0.0171\n",
      "Epoch: 072, Loss: 0.0161\n",
      "Epoch: 073, Loss: 0.0157\n",
      "Epoch: 074, Loss: 0.0154\n",
      "Epoch: 075, Loss: 0.0146\n",
      "Epoch: 076, Loss: 0.0144\n",
      "Epoch: 077, Loss: 0.0137\n",
      "Epoch: 078, Loss: 0.0132\n",
      "Epoch: 079, Loss: 0.0130\n",
      "Epoch: 080, Loss: 0.0123\n",
      "Epoch: 081, Loss: 0.0117\n",
      "Epoch: 082, Loss: 0.0115\n",
      "Epoch: 083, Loss: 0.0110\n",
      "Epoch: 084, Loss: 0.0103\n",
      "Epoch: 085, Loss: 0.0102\n",
      "Epoch: 086, Loss: 0.0099\n",
      "Epoch: 087, Loss: 0.0094\n",
      "Epoch: 088, Loss: 0.0090\n",
      "Epoch: 089, Loss: 0.0088\n",
      "Epoch: 090, Loss: 0.0086\n",
      "Epoch: 091, Loss: 0.0084\n",
      "Epoch: 092, Loss: 0.0080\n",
      "Epoch: 093, Loss: 0.0077\n",
      "Epoch: 094, Loss: 0.0074\n",
      "Epoch: 095, Loss: 0.0072\n",
      "Epoch: 096, Loss: 0.0071\n",
      "Epoch: 097, Loss: 0.0069\n",
      "Epoch: 098, Loss: 0.0067\n",
      "Epoch: 099, Loss: 0.0066\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "data = data.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.mse_loss(torch.squeeze(out[data.train_mask]), data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0136\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "pred = model(data)\n",
    "loss = F.mse_loss(torch.squeeze(pred[data.train_mask]), data.y[data.train_mask])\n",
    "print(f'Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0190\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "pred = model(data)\n",
    "loss = F.mse_loss(torch.squeeze(pred[data.test_mask]), data.y[data.test_mask])\n",
    "print(f'Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, 16)\n",
    "        self.conv2 = GCNConv(16, dataset.num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6010\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0,    0,    0,  ..., 2707, 2707, 2707],\n",
       "        [ 633, 1862, 2582,  ...,  598, 1473, 2706]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0].edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GCN().to(device)\n",
    "data = dataset[0].to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7980\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "pred = model(data).argmax(dim=1)\n",
    "correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
    "acc = int(correct) / int(data.test_mask.sum())\n",
    "print(f'Accuracy: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (substructureML)",
   "language": "python",
   "name": "substructureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
