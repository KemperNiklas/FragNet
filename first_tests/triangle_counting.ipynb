{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mniklas-kemper\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "enable_wandb = True\n",
    "if enable_wandb:\n",
    "    import wandb\n",
    "    wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "import graph_tool as gt\n",
    "import graph_tool.generation as gen\n",
    "import graph_tool.topology as top\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Planetoid(root='/tmp/Cora', name='Cora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rings(edge_index, max_k):\n",
    "    if isinstance(edge_index, torch.Tensor):\n",
    "        edge_index = edge_index.numpy()\n",
    "\n",
    "    edge_list = edge_index.T\n",
    "    graph_gt = gt.Graph(directed=False)\n",
    "    graph_gt.add_edge_list(edge_list)\n",
    "    gen.remove_self_loops(graph_gt)\n",
    "    gen.remove_parallel_edges(graph_gt)\n",
    "    rings = set()\n",
    "    sorted_rings = set()\n",
    "    for k in range(3, max_k+1):\n",
    "        pattern = nx.cycle_graph(k)\n",
    "        pattern_edge_list = list(pattern.edges)\n",
    "        pattern_gt = gt.Graph(directed=False)\n",
    "        pattern_gt.add_edge_list(pattern_edge_list)\n",
    "        sub_isos = top.subgraph_isomorphism(pattern_gt, graph_gt, induced=True, subgraph=True,\n",
    "                                           generator=True)\n",
    "        sub_iso_sets = map(lambda isomorphism: tuple(isomorphism.a), sub_isos)\n",
    "        for iso in sub_iso_sets:\n",
    "            if tuple(sorted(iso)) not in sorted_rings:\n",
    "                rings.add(iso)\n",
    "                sorted_rings.add(tuple(sorted(iso)))\n",
    "    rings = list(rings)\n",
    "    return rings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "triangles = get_rings(dataset[0].edge_index, max_k = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "triangle_count = torch.tensor([list(chain(*triangles)).count(i) for i in range(dataset[0].num_nodes)], dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "substructure_edge_index = torch.tensor([[node_id, triangle_id]  for triangle_id, triangle in enumerate(triangles) for node_id in triangle], dtype = torch.long).t().contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset[0].update({\"substructure_edge_index\": substructure_edge_index, \"y\": triangle_count})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/niklas/Documents/Studium/Informatik/Masterarbeit/substructureML/first_tests/wandb/run-20230713_133503-334myyz5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/niklas-kemper/simple-trinagle-counting/runs/334myyz5' target=\"_blank\">ruby-dragon-6</a></strong> to <a href='https://wandb.ai/niklas-kemper/simple-trinagle-counting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/niklas-kemper/simple-trinagle-counting' target=\"_blank\">https://wandb.ai/niklas-kemper/simple-trinagle-counting</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/niklas-kemper/simple-trinagle-counting/runs/334myyz5' target=\"_blank\">https://wandb.ai/niklas-kemper/simple-trinagle-counting/runs/334myyz5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if enable_wandb:\n",
    "    wandb.init(project='simple-trinagle-counting')\n",
    "    summary = dict()\n",
    "    summary[\"data\"] = dict()\n",
    "    summary[\"data\"][\"num_features\"] = data.num_features\n",
    "    summary[\"data\"][\"num_nodes\"] = data.num_nodes\n",
    "    summary[\"data\"][\"num_edges\"] = data.num_edges \n",
    "    summary[\"data\"][\"has_isolated_nodes\"] = data.has_isolated_nodes()\n",
    "    summary[\"data\"][\"has_self_nodes\"] = data.has_self_loops()\n",
    "    summary[\"data\"][\"is_undirected\"] = data.is_undirected()\n",
    "    summary[\"data\"][\"num_training_nodes\"] = data.train_mask.sum()\n",
    "    wandb.summary = summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simple_model_triangles import SimpleSubstructureNeuralNet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleSubstructureNeuralNet(\n",
      "  (layers): ModuleList(\n",
      "    (0): SubstructureLayer(\n",
      "      (message_neighbor): GINConv(nn=Sequential(\n",
      "        (0): Linear(in_features=1433, out_features=32, bias=True)\n",
      "        (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): Linear(in_features=32, out_features=16, bias=True)\n",
      "      ))\n",
      "      (node2substructure): Sequential(\n",
      "        (0): Linear(in_features=16, out_features=32, bias=True)\n",
      "        (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): Linear(in_features=32, out_features=16, bias=True)\n",
      "      )\n",
      "      (substructure2node): Sequential(\n",
      "        (0): Linear(in_features=16, out_features=32, bias=True)\n",
      "        (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): Linear(in_features=32, out_features=16, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (1-2): 2 x SubstructureLayer(\n",
      "      (message_neighbor): GINConv(nn=Sequential(\n",
      "        (0): Linear(in_features=16, out_features=32, bias=True)\n",
      "        (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): Linear(in_features=32, out_features=16, bias=True)\n",
      "      ))\n",
      "      (node2substructure): Sequential(\n",
      "        (0): Linear(in_features=16, out_features=32, bias=True)\n",
      "        (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): Linear(in_features=32, out_features=16, bias=True)\n",
      "      )\n",
      "      (substructure2node): Sequential(\n",
      "        (0): Linear(in_features=16, out_features=32, bias=True)\n",
      "        (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): Linear(in_features=32, out_features=16, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lin): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = SimpleSubstructureNeuralNet(hidden_channels = 16, out_channels = 1,in_channels = dataset.num_node_features, num_layers = 3)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Embedding, ModuleList\n",
    "from torch.nn import Sequential, Linear, BatchNorm1d, ReLU\n",
    "#from torch_scatter import scatter\n",
    "from torch_geometric.nn import GINConv, GINEConv\n",
    "\n",
    "class SimpleGraphNeuralNet(torch.nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels, out_channels, num_layers):\n",
    "        \n",
    "        super(SimpleGraphNeuralNet, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.node_convs = ModuleList()\n",
    "\n",
    "        for layer in range(num_layers):\n",
    "            if layer == 0:\n",
    "                nn = Sequential(\n",
    "                Linear(input_channels, 2 * hidden_channels),\n",
    "                BatchNorm1d(2 * hidden_channels),\n",
    "                ReLU(),\n",
    "                Linear(2 * hidden_channels, hidden_channels),\n",
    "            )\n",
    "            else:\n",
    "                nn = Sequential(\n",
    "                    Linear(hidden_channels, 2 * hidden_channels),\n",
    "                    BatchNorm1d(2 * hidden_channels),\n",
    "                    ReLU(),\n",
    "                    Linear(2 * hidden_channels, hidden_channels),\n",
    "                )\n",
    "            self.node_convs.append(GINConv(nn, train_eps=True))\n",
    "\n",
    "        self.lin = Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "\n",
    "        for conv in self.node_convs:\n",
    "            conv.reset_parameters()\n",
    "\n",
    "        self.lin.reset_parameters()\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "\n",
    "            x = self.node_convs[i](x=x, edge_index=data.edge_index)\n",
    "\n",
    "\n",
    "        #x = scatter(x, data.batch, dim=0, reduce='mean')\n",
    "        #x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.lin(x)\n",
    "\n",
    "\n",
    "        #x = F.relu(x)\n",
    "        #x = F.dropout(x, self.dropout, training=self.training)\n",
    "        #x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "model2 = SimpleGraphNeuralNet(dataset.num_node_features, 16, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 23.2099, Test Loss: 5.5460, Test Acc: 0.4040\n",
      "Epoch: 002, Loss: 17.4002, Test Loss: 17.9339, Test Acc: 0.2480\n",
      "Epoch: 003, Loss: 11.7592, Test Loss: 16.5006, Test Acc: 0.2530\n",
      "Epoch: 004, Loss: 8.5663, Test Loss: 38.5124, Test Acc: 0.1800\n",
      "Epoch: 005, Loss: 5.4140, Test Loss: 107.5160, Test Acc: 0.0970\n",
      "Epoch: 006, Loss: 5.3247, Test Loss: 183.9236, Test Acc: 0.0690\n",
      "Epoch: 007, Loss: 4.4728, Test Loss: 158.0455, Test Acc: 0.0780\n",
      "Epoch: 008, Loss: 3.3241, Test Loss: 148.4008, Test Acc: 0.0810\n",
      "Epoch: 009, Loss: 3.5670, Test Loss: 139.0910, Test Acc: 0.0750\n",
      "Epoch: 010, Loss: 2.7312, Test Loss: 129.9176, Test Acc: 0.0690\n",
      "Epoch: 011, Loss: 2.9345, Test Loss: 108.9449, Test Acc: 0.0710\n",
      "Epoch: 012, Loss: 2.7284, Test Loss: 90.6493, Test Acc: 0.0780\n",
      "Epoch: 013, Loss: 2.4545, Test Loss: 75.6114, Test Acc: 0.0770\n",
      "Epoch: 014, Loss: 2.3386, Test Loss: 61.7394, Test Acc: 0.0730\n",
      "Epoch: 015, Loss: 2.1019, Test Loss: 50.2313, Test Acc: 0.0680\n",
      "Epoch: 016, Loss: 1.9374, Test Loss: 41.7523, Test Acc: 0.0710\n",
      "Epoch: 017, Loss: 1.8159, Test Loss: 36.6245, Test Acc: 0.0690\n",
      "Epoch: 018, Loss: 1.5667, Test Loss: 34.3206, Test Acc: 0.0670\n",
      "Epoch: 019, Loss: 1.4351, Test Loss: 29.3805, Test Acc: 0.0660\n",
      "Epoch: 020, Loss: 1.3085, Test Loss: 22.3646, Test Acc: 0.0890\n",
      "Epoch: 021, Loss: 1.0596, Test Loss: 17.1101, Test Acc: 0.0960\n",
      "Epoch: 022, Loss: 0.9616, Test Loss: 14.2718, Test Acc: 0.1060\n",
      "Epoch: 023, Loss: 0.9192, Test Loss: 13.1447, Test Acc: 0.1200\n",
      "Epoch: 024, Loss: 0.8307, Test Loss: 13.4828, Test Acc: 0.1220\n",
      "Epoch: 025, Loss: 0.6939, Test Loss: 14.3927, Test Acc: 0.1240\n",
      "Epoch: 026, Loss: 0.6655, Test Loss: 14.1730, Test Acc: 0.1260\n",
      "Epoch: 027, Loss: 0.6653, Test Loss: 12.4336, Test Acc: 0.1400\n",
      "Epoch: 028, Loss: 0.5993, Test Loss: 10.4892, Test Acc: 0.1480\n",
      "Epoch: 029, Loss: 0.5608, Test Loss: 9.0234, Test Acc: 0.1570\n",
      "Epoch: 030, Loss: 0.5353, Test Loss: 8.0759, Test Acc: 0.1650\n",
      "Epoch: 031, Loss: 0.5097, Test Loss: 7.4954, Test Acc: 0.1910\n",
      "Epoch: 032, Loss: 0.4895, Test Loss: 7.1751, Test Acc: 0.2740\n",
      "Epoch: 033, Loss: 0.4478, Test Loss: 6.9286, Test Acc: 0.3290\n",
      "Epoch: 034, Loss: 0.4175, Test Loss: 6.5689, Test Acc: 0.3460\n",
      "Epoch: 035, Loss: 0.4129, Test Loss: 6.0692, Test Acc: 0.3600\n",
      "Epoch: 036, Loss: 0.3938, Test Loss: 5.6032, Test Acc: 0.3690\n",
      "Epoch: 037, Loss: 0.3612, Test Loss: 5.3037, Test Acc: 0.3850\n",
      "Epoch: 038, Loss: 0.3484, Test Loss: 5.1866, Test Acc: 0.3900\n",
      "Epoch: 039, Loss: 0.3475, Test Loss: 5.2159, Test Acc: 0.4000\n",
      "Epoch: 040, Loss: 0.3263, Test Loss: 5.3302, Test Acc: 0.4000\n",
      "Epoch: 041, Loss: 0.3024, Test Loss: 5.4563, Test Acc: 0.3970\n",
      "Epoch: 042, Loss: 0.2975, Test Loss: 5.5126, Test Acc: 0.4000\n",
      "Epoch: 043, Loss: 0.2955, Test Loss: 5.4484, Test Acc: 0.3980\n",
      "Epoch: 044, Loss: 0.2822, Test Loss: 5.2709, Test Acc: 0.4010\n",
      "Epoch: 045, Loss: 0.2674, Test Loss: 5.0485, Test Acc: 0.4080\n",
      "Epoch: 046, Loss: 0.2591, Test Loss: 4.8578, Test Acc: 0.4260\n",
      "Epoch: 047, Loss: 0.2523, Test Loss: 4.7432, Test Acc: 0.4340\n",
      "Epoch: 048, Loss: 0.2437, Test Loss: 4.7012, Test Acc: 0.4380\n",
      "Epoch: 049, Loss: 0.2339, Test Loss: 4.6942, Test Acc: 0.4410\n",
      "Epoch: 050, Loss: 0.2250, Test Loss: 4.6692, Test Acc: 0.4450\n",
      "Epoch: 051, Loss: 0.2178, Test Loss: 4.5898, Test Acc: 0.4440\n",
      "Epoch: 052, Loss: 0.2090, Test Loss: 4.4683, Test Acc: 0.4500\n",
      "Epoch: 053, Loss: 0.1994, Test Loss: 4.3464, Test Acc: 0.4450\n",
      "Epoch: 054, Loss: 0.1936, Test Loss: 4.2585, Test Acc: 0.4510\n",
      "Epoch: 055, Loss: 0.1892, Test Loss: 4.2143, Test Acc: 0.4540\n",
      "Epoch: 056, Loss: 0.1817, Test Loss: 4.1984, Test Acc: 0.4540\n",
      "Epoch: 057, Loss: 0.1750, Test Loss: 4.1846, Test Acc: 0.4550\n",
      "Epoch: 058, Loss: 0.1713, Test Loss: 4.1522, Test Acc: 0.4590\n",
      "Epoch: 059, Loss: 0.1662, Test Loss: 4.1038, Test Acc: 0.4610\n",
      "Epoch: 060, Loss: 0.1590, Test Loss: 4.0621, Test Acc: 0.4600\n",
      "Epoch: 061, Loss: 0.1537, Test Loss: 4.0461, Test Acc: 0.4640\n",
      "Epoch: 062, Loss: 0.1499, Test Loss: 4.0576, Test Acc: 0.4680\n",
      "Epoch: 063, Loss: 0.1456, Test Loss: 4.0836, Test Acc: 0.4740\n",
      "Epoch: 064, Loss: 0.1406, Test Loss: 4.1030, Test Acc: 0.4750\n",
      "Epoch: 065, Loss: 0.1357, Test Loss: 4.1043, Test Acc: 0.4780\n",
      "Epoch: 066, Loss: 0.1316, Test Loss: 4.0903, Test Acc: 0.4770\n",
      "Epoch: 067, Loss: 0.1271, Test Loss: 4.0715, Test Acc: 0.4750\n",
      "Epoch: 068, Loss: 0.1222, Test Loss: 4.0571, Test Acc: 0.4800\n",
      "Epoch: 069, Loss: 0.1183, Test Loss: 4.0497, Test Acc: 0.4850\n",
      "Epoch: 070, Loss: 0.1148, Test Loss: 4.0441, Test Acc: 0.4840\n",
      "Epoch: 071, Loss: 0.1104, Test Loss: 4.0371, Test Acc: 0.4860\n",
      "Epoch: 072, Loss: 0.1064, Test Loss: 4.0258, Test Acc: 0.4860\n",
      "Epoch: 073, Loss: 0.1034, Test Loss: 4.0121, Test Acc: 0.4860\n",
      "Epoch: 074, Loss: 0.0999, Test Loss: 3.9991, Test Acc: 0.4880\n",
      "Epoch: 075, Loss: 0.0964, Test Loss: 3.9917, Test Acc: 0.4900\n",
      "Epoch: 076, Loss: 0.0937, Test Loss: 3.9950, Test Acc: 0.4900\n",
      "Epoch: 077, Loss: 0.0907, Test Loss: 4.0099, Test Acc: 0.4890\n",
      "Epoch: 078, Loss: 0.0871, Test Loss: 4.0337, Test Acc: 0.4880\n",
      "Epoch: 079, Loss: 0.0842, Test Loss: 4.0601, Test Acc: 0.4840\n",
      "Epoch: 080, Loss: 0.0813, Test Loss: 4.0817, Test Acc: 0.4830\n",
      "Epoch: 081, Loss: 0.0782, Test Loss: 4.0983, Test Acc: 0.4810\n",
      "Epoch: 082, Loss: 0.0756, Test Loss: 4.1104, Test Acc: 0.4810\n",
      "Epoch: 083, Loss: 0.0732, Test Loss: 4.1230, Test Acc: 0.4820\n",
      "Epoch: 084, Loss: 0.0705, Test Loss: 4.1364, Test Acc: 0.4830\n",
      "Epoch: 085, Loss: 0.0679, Test Loss: 4.1459, Test Acc: 0.4810\n",
      "Epoch: 086, Loss: 0.0655, Test Loss: 4.1477, Test Acc: 0.4800\n",
      "Epoch: 087, Loss: 0.0629, Test Loss: 4.1468, Test Acc: 0.4780\n",
      "Epoch: 088, Loss: 0.0604, Test Loss: 4.1464, Test Acc: 0.4780\n",
      "Epoch: 089, Loss: 0.0583, Test Loss: 4.1500, Test Acc: 0.4760\n",
      "Epoch: 090, Loss: 0.0562, Test Loss: 4.1589, Test Acc: 0.4750\n",
      "Epoch: 091, Loss: 0.0541, Test Loss: 4.1697, Test Acc: 0.4750\n",
      "Epoch: 092, Loss: 0.0522, Test Loss: 4.1782, Test Acc: 0.4760\n",
      "Epoch: 093, Loss: 0.0503, Test Loss: 4.1839, Test Acc: 0.4760\n",
      "Epoch: 094, Loss: 0.0485, Test Loss: 4.1906, Test Acc: 0.4770\n",
      "Epoch: 095, Loss: 0.0469, Test Loss: 4.2009, Test Acc: 0.4770\n",
      "Epoch: 096, Loss: 0.0453, Test Loss: 4.2130, Test Acc: 0.4740\n",
      "Epoch: 097, Loss: 0.0437, Test Loss: 4.2232, Test Acc: 0.4770\n",
      "Epoch: 098, Loss: 0.0423, Test Loss: 4.2282, Test Acc: 0.4770\n",
      "Epoch: 099, Loss: 0.0408, Test Loss: 4.2314, Test Acc: 0.4750\n",
      "Epoch: 100, Loss: 0.0395, Test Loss: 4.2364, Test Acc: 0.4750\n",
      "Epoch: 101, Loss: 0.0382, Test Loss: 4.2444, Test Acc: 0.4750\n",
      "Epoch: 102, Loss: 0.0370, Test Loss: 4.2524, Test Acc: 0.4750\n",
      "Epoch: 103, Loss: 0.0358, Test Loss: 4.2572, Test Acc: 0.4750\n",
      "Epoch: 104, Loss: 0.0345, Test Loss: 4.2606, Test Acc: 0.4730\n",
      "Epoch: 105, Loss: 0.0333, Test Loss: 4.2660, Test Acc: 0.4720\n",
      "Epoch: 106, Loss: 0.0321, Test Loss: 4.2744, Test Acc: 0.4750\n",
      "Epoch: 107, Loss: 0.0309, Test Loss: 4.2842, Test Acc: 0.4740\n",
      "Epoch: 108, Loss: 0.0297, Test Loss: 4.2925, Test Acc: 0.4750\n",
      "Epoch: 109, Loss: 0.0286, Test Loss: 4.2983, Test Acc: 0.4780\n",
      "Epoch: 110, Loss: 0.0275, Test Loss: 4.3043, Test Acc: 0.4790\n",
      "Epoch: 111, Loss: 0.0265, Test Loss: 4.3108, Test Acc: 0.4790\n",
      "Epoch: 112, Loss: 0.0257, Test Loss: 4.3180, Test Acc: 0.4790\n",
      "Epoch: 113, Loss: 0.0248, Test Loss: 4.3263, Test Acc: 0.4800\n",
      "Epoch: 114, Loss: 0.0240, Test Loss: 4.3362, Test Acc: 0.4810\n",
      "Epoch: 115, Loss: 0.0231, Test Loss: 4.3479, Test Acc: 0.4800\n",
      "Epoch: 116, Loss: 0.0221, Test Loss: 4.3603, Test Acc: 0.4790\n",
      "Epoch: 117, Loss: 0.0211, Test Loss: 4.3730, Test Acc: 0.4770\n",
      "Epoch: 118, Loss: 0.0201, Test Loss: 4.3851, Test Acc: 0.4780\n",
      "Epoch: 119, Loss: 0.0191, Test Loss: 4.3975, Test Acc: 0.4800\n",
      "Epoch: 120, Loss: 0.0181, Test Loss: 4.4100, Test Acc: 0.4810\n",
      "Epoch: 121, Loss: 0.0172, Test Loss: 4.4211, Test Acc: 0.4800\n",
      "Epoch: 122, Loss: 0.0163, Test Loss: 4.4313, Test Acc: 0.4780\n",
      "Epoch: 123, Loss: 0.0155, Test Loss: 4.4411, Test Acc: 0.4780\n",
      "Epoch: 124, Loss: 0.0147, Test Loss: 4.4501, Test Acc: 0.4790\n",
      "Epoch: 125, Loss: 0.0141, Test Loss: 4.4589, Test Acc: 0.4790\n",
      "Epoch: 126, Loss: 0.0134, Test Loss: 4.4680, Test Acc: 0.4780\n",
      "Epoch: 127, Loss: 0.0128, Test Loss: 4.4784, Test Acc: 0.4770\n",
      "Epoch: 128, Loss: 0.0123, Test Loss: 4.4892, Test Acc: 0.4770\n",
      "Epoch: 129, Loss: 0.0118, Test Loss: 4.5000, Test Acc: 0.4760\n",
      "Epoch: 130, Loss: 0.0113, Test Loss: 4.5106, Test Acc: 0.4770\n",
      "Epoch: 131, Loss: 0.0109, Test Loss: 4.5219, Test Acc: 0.4790\n",
      "Epoch: 132, Loss: 0.0105, Test Loss: 4.5338, Test Acc: 0.4800\n",
      "Epoch: 133, Loss: 0.0101, Test Loss: 4.5452, Test Acc: 0.4800\n",
      "Epoch: 134, Loss: 0.0098, Test Loss: 4.5557, Test Acc: 0.4800\n",
      "Epoch: 135, Loss: 0.0095, Test Loss: 4.5646, Test Acc: 0.4800\n",
      "Epoch: 136, Loss: 0.0092, Test Loss: 4.5723, Test Acc: 0.4800\n",
      "Epoch: 137, Loss: 0.0089, Test Loss: 4.5789, Test Acc: 0.4810\n",
      "Epoch: 138, Loss: 0.0086, Test Loss: 4.5846, Test Acc: 0.4820\n",
      "Epoch: 139, Loss: 0.0084, Test Loss: 4.5907, Test Acc: 0.4830\n",
      "Epoch: 140, Loss: 0.0081, Test Loss: 4.5979, Test Acc: 0.4810\n",
      "Epoch: 141, Loss: 0.0079, Test Loss: 4.6067, Test Acc: 0.4810\n",
      "Epoch: 142, Loss: 0.0077, Test Loss: 4.6156, Test Acc: 0.4820\n",
      "Epoch: 143, Loss: 0.0075, Test Loss: 4.6237, Test Acc: 0.4810\n",
      "Epoch: 144, Loss: 0.0073, Test Loss: 4.6315, Test Acc: 0.4810\n",
      "Epoch: 145, Loss: 0.0071, Test Loss: 4.6394, Test Acc: 0.4810\n",
      "Epoch: 146, Loss: 0.0069, Test Loss: 4.6461, Test Acc: 0.4810\n",
      "Epoch: 147, Loss: 0.0068, Test Loss: 4.6519, Test Acc: 0.4810\n",
      "Epoch: 148, Loss: 0.0066, Test Loss: 4.6574, Test Acc: 0.4810\n",
      "Epoch: 149, Loss: 0.0064, Test Loss: 4.6626, Test Acc: 0.4810\n",
      "Epoch: 150, Loss: 0.0063, Test Loss: 4.6680, Test Acc: 0.4800\n",
      "Epoch: 151, Loss: 0.0061, Test Loss: 4.6739, Test Acc: 0.4800\n",
      "Epoch: 152, Loss: 0.0060, Test Loss: 4.6794, Test Acc: 0.4800\n",
      "Epoch: 153, Loss: 0.0059, Test Loss: 4.6835, Test Acc: 0.4810\n",
      "Epoch: 154, Loss: 0.0057, Test Loss: 4.6865, Test Acc: 0.4810\n",
      "Epoch: 155, Loss: 0.0056, Test Loss: 4.6900, Test Acc: 0.4810\n",
      "Epoch: 156, Loss: 0.0055, Test Loss: 4.6927, Test Acc: 0.4800\n",
      "Epoch: 157, Loss: 0.0054, Test Loss: 4.6947, Test Acc: 0.4790\n",
      "Epoch: 158, Loss: 0.0053, Test Loss: 4.6982, Test Acc: 0.4800\n",
      "Epoch: 159, Loss: 0.0052, Test Loss: 4.7023, Test Acc: 0.4800\n",
      "Epoch: 160, Loss: 0.0051, Test Loss: 4.7057, Test Acc: 0.4800\n",
      "Epoch: 161, Loss: 0.0050, Test Loss: 4.7090, Test Acc: 0.4800\n",
      "Epoch: 162, Loss: 0.0049, Test Loss: 4.7115, Test Acc: 0.4800\n",
      "Epoch: 163, Loss: 0.0048, Test Loss: 4.7135, Test Acc: 0.4810\n",
      "Epoch: 164, Loss: 0.0047, Test Loss: 4.7158, Test Acc: 0.4800\n",
      "Epoch: 165, Loss: 0.0046, Test Loss: 4.7182, Test Acc: 0.4800\n",
      "Epoch: 166, Loss: 0.0045, Test Loss: 4.7212, Test Acc: 0.4810\n",
      "Epoch: 167, Loss: 0.0044, Test Loss: 4.7246, Test Acc: 0.4810\n",
      "Epoch: 168, Loss: 0.0043, Test Loss: 4.7281, Test Acc: 0.4820\n",
      "Epoch: 169, Loss: 0.0043, Test Loss: 4.7321, Test Acc: 0.4810\n",
      "Epoch: 170, Loss: 0.0042, Test Loss: 4.7361, Test Acc: 0.4810\n",
      "Epoch: 171, Loss: 0.0041, Test Loss: 4.7393, Test Acc: 0.4810\n",
      "Epoch: 172, Loss: 0.0040, Test Loss: 4.7428, Test Acc: 0.4810\n",
      "Epoch: 173, Loss: 0.0040, Test Loss: 4.7473, Test Acc: 0.4810\n",
      "Epoch: 174, Loss: 0.0039, Test Loss: 4.7520, Test Acc: 0.4800\n",
      "Epoch: 175, Loss: 0.0038, Test Loss: 4.7566, Test Acc: 0.4810\n",
      "Epoch: 176, Loss: 0.0038, Test Loss: 4.7609, Test Acc: 0.4810\n",
      "Epoch: 177, Loss: 0.0037, Test Loss: 4.7651, Test Acc: 0.4810\n",
      "Epoch: 178, Loss: 0.0036, Test Loss: 4.7692, Test Acc: 0.4800\n",
      "Epoch: 179, Loss: 0.0036, Test Loss: 4.7736, Test Acc: 0.4810\n",
      "Epoch: 180, Loss: 0.0035, Test Loss: 4.7787, Test Acc: 0.4790\n",
      "Epoch: 181, Loss: 0.0035, Test Loss: 4.7844, Test Acc: 0.4810\n",
      "Epoch: 182, Loss: 0.0034, Test Loss: 4.7908, Test Acc: 0.4830\n",
      "Epoch: 183, Loss: 0.0034, Test Loss: 4.7977, Test Acc: 0.4830\n",
      "Epoch: 184, Loss: 0.0033, Test Loss: 4.8044, Test Acc: 0.4830\n",
      "Epoch: 185, Loss: 0.0033, Test Loss: 4.8109, Test Acc: 0.4820\n",
      "Epoch: 186, Loss: 0.0032, Test Loss: 4.8174, Test Acc: 0.4820\n",
      "Epoch: 187, Loss: 0.0032, Test Loss: 4.8237, Test Acc: 0.4810\n",
      "Epoch: 188, Loss: 0.0031, Test Loss: 4.8302, Test Acc: 0.4810\n",
      "Epoch: 189, Loss: 0.0031, Test Loss: 4.8366, Test Acc: 0.4810\n",
      "Epoch: 190, Loss: 0.0030, Test Loss: 4.8436, Test Acc: 0.4810\n",
      "Epoch: 191, Loss: 0.0030, Test Loss: 4.8504, Test Acc: 0.4810\n",
      "Epoch: 192, Loss: 0.0029, Test Loss: 4.8580, Test Acc: 0.4810\n",
      "Epoch: 193, Loss: 0.0029, Test Loss: 4.8644, Test Acc: 0.4810\n",
      "Epoch: 194, Loss: 0.0029, Test Loss: 4.8708, Test Acc: 0.4820\n",
      "Epoch: 195, Loss: 0.0028, Test Loss: 4.8770, Test Acc: 0.4810\n",
      "Epoch: 196, Loss: 0.0028, Test Loss: 4.8851, Test Acc: 0.4820\n",
      "Epoch: 197, Loss: 0.0027, Test Loss: 4.8904, Test Acc: 0.4820\n",
      "Epoch: 198, Loss: 0.0027, Test Loss: 4.9000, Test Acc: 0.4810\n",
      "Epoch: 199, Loss: 0.0027, Test Loss: 4.9043, Test Acc: 0.4810\n",
      "Epoch: 200, Loss: 0.0027, Test Loss: 4.9157, Test Acc: 0.4810\n"
     ]
    }
   ],
   "source": [
    "criterion = F.mse_loss  # Define loss criterion.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)  # Define optimizer.\n",
    "\n",
    "def train():\n",
    "      model.train()\n",
    "      optimizer.zero_grad()  # Clear gradients.\n",
    "      out = torch.squeeze(model(data))  # Perform a single forward pass.\n",
    "      loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss solely based on the training nodes.\n",
    "      loss.backward()  # Derive gradients.\n",
    "      optimizer.step()  # Update parameters based on gradients.\n",
    "      return loss\n",
    "\n",
    "def test():\n",
    "      model.eval()\n",
    "      out = torch.squeeze(model(data))\n",
    "      pred = torch.round(out)  # Round to integer\n",
    "      test_loss = criterion(out[data.test_mask], data.y[data.test_mask])\n",
    "      test_correct = pred[data.test_mask] == data.y[data.test_mask]  # Check against ground-truth labels.\n",
    "      test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  # Derive ratio of correct predictions.\n",
    "      return test_acc, test_loss\n",
    "\n",
    "#model.reset_parameters()\n",
    "for epoch in range(1, 201):\n",
    "    loss = train()\n",
    "    test_acc, test_loss = test()\n",
    "    if enable_wandb:\n",
    "        wandb.log({\"substructureGNN/loss\": loss, \"substructureGNN/test_loss\": test_loss, \"substructureGNN/test_acc\": test_acc})\n",
    "    \n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Loss: 24.1846\n",
      "Epoch: 001, Loss: 21.5803\n",
      "Epoch: 002, Loss: 18.1493\n",
      "Epoch: 003, Loss: 13.4987\n",
      "Epoch: 004, Loss: 10.6048\n",
      "Epoch: 005, Loss: 7.6783\n",
      "Epoch: 006, Loss: 4.2272\n",
      "Epoch: 007, Loss: 1.4557\n",
      "Epoch: 008, Loss: 1.3588\n",
      "Epoch: 009, Loss: 1.6830\n",
      "Epoch: 010, Loss: 1.8849\n",
      "Epoch: 011, Loss: 1.7098\n",
      "Epoch: 012, Loss: 1.4532\n",
      "Epoch: 013, Loss: 1.2828\n",
      "Epoch: 014, Loss: 1.0259\n",
      "Epoch: 015, Loss: 0.8474\n",
      "Epoch: 016, Loss: 0.6811\n",
      "Epoch: 017, Loss: 0.6298\n",
      "Epoch: 018, Loss: 0.5018\n",
      "Epoch: 019, Loss: 0.4021\n",
      "Epoch: 020, Loss: 0.3520\n",
      "Epoch: 021, Loss: 0.3064\n",
      "Epoch: 022, Loss: 0.2621\n",
      "Epoch: 023, Loss: 0.2384\n",
      "Epoch: 024, Loss: 0.2200\n",
      "Epoch: 025, Loss: 0.2020\n",
      "Epoch: 026, Loss: 0.1927\n",
      "Epoch: 027, Loss: 0.1865\n",
      "Epoch: 028, Loss: 0.1797\n",
      "Epoch: 029, Loss: 0.1806\n",
      "Epoch: 030, Loss: 0.1825\n",
      "Epoch: 031, Loss: 0.1733\n",
      "Epoch: 032, Loss: 0.1699\n",
      "Epoch: 033, Loss: 0.1586\n",
      "Epoch: 034, Loss: 0.1498\n",
      "Epoch: 035, Loss: 0.1391\n",
      "Epoch: 036, Loss: 0.1285\n",
      "Epoch: 037, Loss: 0.1212\n",
      "Epoch: 038, Loss: 0.1097\n",
      "Epoch: 039, Loss: 0.1026\n",
      "Epoch: 040, Loss: 0.0934\n",
      "Epoch: 041, Loss: 0.0893\n",
      "Epoch: 042, Loss: 0.0820\n",
      "Epoch: 043, Loss: 0.0783\n",
      "Epoch: 044, Loss: 0.0729\n",
      "Epoch: 045, Loss: 0.0710\n",
      "Epoch: 046, Loss: 0.0650\n",
      "Epoch: 047, Loss: 0.0595\n",
      "Epoch: 048, Loss: 0.0578\n",
      "Epoch: 049, Loss: 0.0538\n",
      "Epoch: 050, Loss: 0.0517\n",
      "Epoch: 051, Loss: 0.0483\n",
      "Epoch: 052, Loss: 0.0486\n",
      "Epoch: 053, Loss: 0.0443\n",
      "Epoch: 054, Loss: 0.0391\n",
      "Epoch: 055, Loss: 0.0391\n",
      "Epoch: 056, Loss: 0.0357\n",
      "Epoch: 057, Loss: 0.0347\n",
      "Epoch: 058, Loss: 0.0326\n",
      "Epoch: 059, Loss: 0.0305\n",
      "Epoch: 060, Loss: 0.0291\n",
      "Epoch: 061, Loss: 0.0269\n",
      "Epoch: 062, Loss: 0.0265\n",
      "Epoch: 063, Loss: 0.0244\n",
      "Epoch: 064, Loss: 0.0241\n",
      "Epoch: 065, Loss: 0.0221\n",
      "Epoch: 066, Loss: 0.0214\n",
      "Epoch: 067, Loss: 0.0203\n",
      "Epoch: 068, Loss: 0.0193\n",
      "Epoch: 069, Loss: 0.0189\n",
      "Epoch: 070, Loss: 0.0175\n",
      "Epoch: 071, Loss: 0.0171\n",
      "Epoch: 072, Loss: 0.0161\n",
      "Epoch: 073, Loss: 0.0157\n",
      "Epoch: 074, Loss: 0.0154\n",
      "Epoch: 075, Loss: 0.0146\n",
      "Epoch: 076, Loss: 0.0144\n",
      "Epoch: 077, Loss: 0.0137\n",
      "Epoch: 078, Loss: 0.0132\n",
      "Epoch: 079, Loss: 0.0130\n",
      "Epoch: 080, Loss: 0.0123\n",
      "Epoch: 081, Loss: 0.0117\n",
      "Epoch: 082, Loss: 0.0115\n",
      "Epoch: 083, Loss: 0.0110\n",
      "Epoch: 084, Loss: 0.0103\n",
      "Epoch: 085, Loss: 0.0102\n",
      "Epoch: 086, Loss: 0.0099\n",
      "Epoch: 087, Loss: 0.0094\n",
      "Epoch: 088, Loss: 0.0090\n",
      "Epoch: 089, Loss: 0.0088\n",
      "Epoch: 090, Loss: 0.0086\n",
      "Epoch: 091, Loss: 0.0084\n",
      "Epoch: 092, Loss: 0.0080\n",
      "Epoch: 093, Loss: 0.0077\n",
      "Epoch: 094, Loss: 0.0074\n",
      "Epoch: 095, Loss: 0.0072\n",
      "Epoch: 096, Loss: 0.0071\n",
      "Epoch: 097, Loss: 0.0069\n",
      "Epoch: 098, Loss: 0.0067\n",
      "Epoch: 099, Loss: 0.0066\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "data = data.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.mse_loss(torch.squeeze(out[data.train_mask]), data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0136\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "pred = model(data)\n",
    "loss = F.mse_loss(torch.squeeze(pred[data.train_mask]), data.y[data.train_mask])\n",
    "print(f'Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0190\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "pred = model(data)\n",
    "loss = F.mse_loss(torch.squeeze(pred[data.test_mask]), data.y[data.test_mask])\n",
    "print(f'Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, 16)\n",
    "        self.conv2 = GCNConv(16, dataset.num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6010\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0,    0,    0,  ..., 2707, 2707, 2707],\n",
       "        [ 633, 1862, 2582,  ...,  598, 1473, 2706]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0].edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GCN().to(device)\n",
    "data = dataset[0].to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7980\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "pred = model(data).argmax(dim=1)\n",
    "correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
    "acc = int(correct) / int(data.test_mask.sum())\n",
    "print(f'Accuracy: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (substructureML)",
   "language": "python",
   "name": "substructureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
